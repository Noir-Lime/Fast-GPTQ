{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoj/Work/Fast-GPTQ/model.py:596: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(self.config.model_path, framework=\"pt\", device=\"cpu\") as f:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_modules at 0x7fd09757a810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import ExLlama, ExLlamaConfig\n",
    "\n",
    "config = ExLlamaConfig(\"wizard-vicuna-13B-GPTQ/config.json\")\n",
    "\n",
    "config.model_path = \"wizard-vicuna-13B-GPTQ/wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\n",
    "\n",
    "model = ExLlama(config)\n",
    "\n",
    "model.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ExLlama(\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      "  (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
      "  (norm): ExLlamaRMSNorm()\n",
      "  (layers): ModuleList(\n",
      "    (0-39): 40 x ExLlamaDecoderLayer(\n",
      "      (self_attn): ExLlamaAttention(\n",
      "        (q_proj): Ex4bitLinear()\n",
      "        (k_proj): Ex4bitLinear()\n",
      "        (v_proj): Ex4bitLinear()\n",
      "        (o_proj): Ex4bitLinear()\n",
      "      )\n",
      "      (mlp): ExLlamaMLP(\n",
      "        (gate_proj): Ex4bitLinear()\n",
      "        (up_proj): Ex4bitLinear()\n",
      "        (down_proj): Ex4bitLinear()\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): ExLlamaRMSNorm()\n",
      "      (post_attention_layernorm): ExLlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "lm_head Linear(in_features=5120, out_features=32000, bias=False)\n",
      "embed_tokens Embedding(32000, 5120, padding_idx=0)\n",
      "norm ExLlamaRMSNorm()\n",
      "layers ModuleList(\n",
      "  (0-39): 40 x ExLlamaDecoderLayer(\n",
      "    (self_attn): ExLlamaAttention(\n",
      "      (q_proj): Ex4bitLinear()\n",
      "      (k_proj): Ex4bitLinear()\n",
      "      (v_proj): Ex4bitLinear()\n",
      "      (o_proj): Ex4bitLinear()\n",
      "    )\n",
      "    (mlp): ExLlamaMLP(\n",
      "      (gate_proj): Ex4bitLinear()\n",
      "      (up_proj): Ex4bitLinear()\n",
      "      (down_proj): Ex4bitLinear()\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): ExLlamaRMSNorm()\n",
      "    (post_attention_layernorm): ExLlamaRMSNorm()\n",
      "  )\n",
      ")\n",
      "layers.0 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.0.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.0.self_attn.q_proj Ex4bitLinear()\n",
      "layers.0.self_attn.k_proj Ex4bitLinear()\n",
      "layers.0.self_attn.v_proj Ex4bitLinear()\n",
      "layers.0.self_attn.o_proj Ex4bitLinear()\n",
      "layers.0.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.0.mlp.gate_proj Ex4bitLinear()\n",
      "layers.0.mlp.up_proj Ex4bitLinear()\n",
      "layers.0.mlp.down_proj Ex4bitLinear()\n",
      "layers.0.mlp.act_fn SiLU()\n",
      "layers.0.input_layernorm ExLlamaRMSNorm()\n",
      "layers.0.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.1 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.1.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.1.self_attn.q_proj Ex4bitLinear()\n",
      "layers.1.self_attn.k_proj Ex4bitLinear()\n",
      "layers.1.self_attn.v_proj Ex4bitLinear()\n",
      "layers.1.self_attn.o_proj Ex4bitLinear()\n",
      "layers.1.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.1.mlp.gate_proj Ex4bitLinear()\n",
      "layers.1.mlp.up_proj Ex4bitLinear()\n",
      "layers.1.mlp.down_proj Ex4bitLinear()\n",
      "layers.1.mlp.act_fn SiLU()\n",
      "layers.1.input_layernorm ExLlamaRMSNorm()\n",
      "layers.1.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.2 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.2.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.2.self_attn.q_proj Ex4bitLinear()\n",
      "layers.2.self_attn.k_proj Ex4bitLinear()\n",
      "layers.2.self_attn.v_proj Ex4bitLinear()\n",
      "layers.2.self_attn.o_proj Ex4bitLinear()\n",
      "layers.2.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.2.mlp.gate_proj Ex4bitLinear()\n",
      "layers.2.mlp.up_proj Ex4bitLinear()\n",
      "layers.2.mlp.down_proj Ex4bitLinear()\n",
      "layers.2.mlp.act_fn SiLU()\n",
      "layers.2.input_layernorm ExLlamaRMSNorm()\n",
      "layers.2.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.3 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.3.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.3.self_attn.q_proj Ex4bitLinear()\n",
      "layers.3.self_attn.k_proj Ex4bitLinear()\n",
      "layers.3.self_attn.v_proj Ex4bitLinear()\n",
      "layers.3.self_attn.o_proj Ex4bitLinear()\n",
      "layers.3.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.3.mlp.gate_proj Ex4bitLinear()\n",
      "layers.3.mlp.up_proj Ex4bitLinear()\n",
      "layers.3.mlp.down_proj Ex4bitLinear()\n",
      "layers.3.mlp.act_fn SiLU()\n",
      "layers.3.input_layernorm ExLlamaRMSNorm()\n",
      "layers.3.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.4 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.4.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.4.self_attn.q_proj Ex4bitLinear()\n",
      "layers.4.self_attn.k_proj Ex4bitLinear()\n",
      "layers.4.self_attn.v_proj Ex4bitLinear()\n",
      "layers.4.self_attn.o_proj Ex4bitLinear()\n",
      "layers.4.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.4.mlp.gate_proj Ex4bitLinear()\n",
      "layers.4.mlp.up_proj Ex4bitLinear()\n",
      "layers.4.mlp.down_proj Ex4bitLinear()\n",
      "layers.4.mlp.act_fn SiLU()\n",
      "layers.4.input_layernorm ExLlamaRMSNorm()\n",
      "layers.4.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.5 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.5.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.5.self_attn.q_proj Ex4bitLinear()\n",
      "layers.5.self_attn.k_proj Ex4bitLinear()\n",
      "layers.5.self_attn.v_proj Ex4bitLinear()\n",
      "layers.5.self_attn.o_proj Ex4bitLinear()\n",
      "layers.5.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.5.mlp.gate_proj Ex4bitLinear()\n",
      "layers.5.mlp.up_proj Ex4bitLinear()\n",
      "layers.5.mlp.down_proj Ex4bitLinear()\n",
      "layers.5.mlp.act_fn SiLU()\n",
      "layers.5.input_layernorm ExLlamaRMSNorm()\n",
      "layers.5.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.6 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.6.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.6.self_attn.q_proj Ex4bitLinear()\n",
      "layers.6.self_attn.k_proj Ex4bitLinear()\n",
      "layers.6.self_attn.v_proj Ex4bitLinear()\n",
      "layers.6.self_attn.o_proj Ex4bitLinear()\n",
      "layers.6.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.6.mlp.gate_proj Ex4bitLinear()\n",
      "layers.6.mlp.up_proj Ex4bitLinear()\n",
      "layers.6.mlp.down_proj Ex4bitLinear()\n",
      "layers.6.mlp.act_fn SiLU()\n",
      "layers.6.input_layernorm ExLlamaRMSNorm()\n",
      "layers.6.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.7 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.7.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.7.self_attn.q_proj Ex4bitLinear()\n",
      "layers.7.self_attn.k_proj Ex4bitLinear()\n",
      "layers.7.self_attn.v_proj Ex4bitLinear()\n",
      "layers.7.self_attn.o_proj Ex4bitLinear()\n",
      "layers.7.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.7.mlp.gate_proj Ex4bitLinear()\n",
      "layers.7.mlp.up_proj Ex4bitLinear()\n",
      "layers.7.mlp.down_proj Ex4bitLinear()\n",
      "layers.7.mlp.act_fn SiLU()\n",
      "layers.7.input_layernorm ExLlamaRMSNorm()\n",
      "layers.7.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.8 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.8.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.8.self_attn.q_proj Ex4bitLinear()\n",
      "layers.8.self_attn.k_proj Ex4bitLinear()\n",
      "layers.8.self_attn.v_proj Ex4bitLinear()\n",
      "layers.8.self_attn.o_proj Ex4bitLinear()\n",
      "layers.8.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.8.mlp.gate_proj Ex4bitLinear()\n",
      "layers.8.mlp.up_proj Ex4bitLinear()\n",
      "layers.8.mlp.down_proj Ex4bitLinear()\n",
      "layers.8.mlp.act_fn SiLU()\n",
      "layers.8.input_layernorm ExLlamaRMSNorm()\n",
      "layers.8.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.9 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.9.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.9.self_attn.q_proj Ex4bitLinear()\n",
      "layers.9.self_attn.k_proj Ex4bitLinear()\n",
      "layers.9.self_attn.v_proj Ex4bitLinear()\n",
      "layers.9.self_attn.o_proj Ex4bitLinear()\n",
      "layers.9.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.9.mlp.gate_proj Ex4bitLinear()\n",
      "layers.9.mlp.up_proj Ex4bitLinear()\n",
      "layers.9.mlp.down_proj Ex4bitLinear()\n",
      "layers.9.mlp.act_fn SiLU()\n",
      "layers.9.input_layernorm ExLlamaRMSNorm()\n",
      "layers.9.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.10 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.10.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.10.self_attn.q_proj Ex4bitLinear()\n",
      "layers.10.self_attn.k_proj Ex4bitLinear()\n",
      "layers.10.self_attn.v_proj Ex4bitLinear()\n",
      "layers.10.self_attn.o_proj Ex4bitLinear()\n",
      "layers.10.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.10.mlp.gate_proj Ex4bitLinear()\n",
      "layers.10.mlp.up_proj Ex4bitLinear()\n",
      "layers.10.mlp.down_proj Ex4bitLinear()\n",
      "layers.10.mlp.act_fn SiLU()\n",
      "layers.10.input_layernorm ExLlamaRMSNorm()\n",
      "layers.10.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.11 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.11.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.11.self_attn.q_proj Ex4bitLinear()\n",
      "layers.11.self_attn.k_proj Ex4bitLinear()\n",
      "layers.11.self_attn.v_proj Ex4bitLinear()\n",
      "layers.11.self_attn.o_proj Ex4bitLinear()\n",
      "layers.11.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.11.mlp.gate_proj Ex4bitLinear()\n",
      "layers.11.mlp.up_proj Ex4bitLinear()\n",
      "layers.11.mlp.down_proj Ex4bitLinear()\n",
      "layers.11.mlp.act_fn SiLU()\n",
      "layers.11.input_layernorm ExLlamaRMSNorm()\n",
      "layers.11.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.12 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.12.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.12.self_attn.q_proj Ex4bitLinear()\n",
      "layers.12.self_attn.k_proj Ex4bitLinear()\n",
      "layers.12.self_attn.v_proj Ex4bitLinear()\n",
      "layers.12.self_attn.o_proj Ex4bitLinear()\n",
      "layers.12.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.12.mlp.gate_proj Ex4bitLinear()\n",
      "layers.12.mlp.up_proj Ex4bitLinear()\n",
      "layers.12.mlp.down_proj Ex4bitLinear()\n",
      "layers.12.mlp.act_fn SiLU()\n",
      "layers.12.input_layernorm ExLlamaRMSNorm()\n",
      "layers.12.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.13 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.13.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.13.self_attn.q_proj Ex4bitLinear()\n",
      "layers.13.self_attn.k_proj Ex4bitLinear()\n",
      "layers.13.self_attn.v_proj Ex4bitLinear()\n",
      "layers.13.self_attn.o_proj Ex4bitLinear()\n",
      "layers.13.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.13.mlp.gate_proj Ex4bitLinear()\n",
      "layers.13.mlp.up_proj Ex4bitLinear()\n",
      "layers.13.mlp.down_proj Ex4bitLinear()\n",
      "layers.13.mlp.act_fn SiLU()\n",
      "layers.13.input_layernorm ExLlamaRMSNorm()\n",
      "layers.13.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.14 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.14.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.14.self_attn.q_proj Ex4bitLinear()\n",
      "layers.14.self_attn.k_proj Ex4bitLinear()\n",
      "layers.14.self_attn.v_proj Ex4bitLinear()\n",
      "layers.14.self_attn.o_proj Ex4bitLinear()\n",
      "layers.14.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.14.mlp.gate_proj Ex4bitLinear()\n",
      "layers.14.mlp.up_proj Ex4bitLinear()\n",
      "layers.14.mlp.down_proj Ex4bitLinear()\n",
      "layers.14.mlp.act_fn SiLU()\n",
      "layers.14.input_layernorm ExLlamaRMSNorm()\n",
      "layers.14.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.15 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.15.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.15.self_attn.q_proj Ex4bitLinear()\n",
      "layers.15.self_attn.k_proj Ex4bitLinear()\n",
      "layers.15.self_attn.v_proj Ex4bitLinear()\n",
      "layers.15.self_attn.o_proj Ex4bitLinear()\n",
      "layers.15.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.15.mlp.gate_proj Ex4bitLinear()\n",
      "layers.15.mlp.up_proj Ex4bitLinear()\n",
      "layers.15.mlp.down_proj Ex4bitLinear()\n",
      "layers.15.mlp.act_fn SiLU()\n",
      "layers.15.input_layernorm ExLlamaRMSNorm()\n",
      "layers.15.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.16 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.16.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.16.self_attn.q_proj Ex4bitLinear()\n",
      "layers.16.self_attn.k_proj Ex4bitLinear()\n",
      "layers.16.self_attn.v_proj Ex4bitLinear()\n",
      "layers.16.self_attn.o_proj Ex4bitLinear()\n",
      "layers.16.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.16.mlp.gate_proj Ex4bitLinear()\n",
      "layers.16.mlp.up_proj Ex4bitLinear()\n",
      "layers.16.mlp.down_proj Ex4bitLinear()\n",
      "layers.16.mlp.act_fn SiLU()\n",
      "layers.16.input_layernorm ExLlamaRMSNorm()\n",
      "layers.16.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.17 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.17.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.17.self_attn.q_proj Ex4bitLinear()\n",
      "layers.17.self_attn.k_proj Ex4bitLinear()\n",
      "layers.17.self_attn.v_proj Ex4bitLinear()\n",
      "layers.17.self_attn.o_proj Ex4bitLinear()\n",
      "layers.17.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.17.mlp.gate_proj Ex4bitLinear()\n",
      "layers.17.mlp.up_proj Ex4bitLinear()\n",
      "layers.17.mlp.down_proj Ex4bitLinear()\n",
      "layers.17.mlp.act_fn SiLU()\n",
      "layers.17.input_layernorm ExLlamaRMSNorm()\n",
      "layers.17.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.18 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.18.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.18.self_attn.q_proj Ex4bitLinear()\n",
      "layers.18.self_attn.k_proj Ex4bitLinear()\n",
      "layers.18.self_attn.v_proj Ex4bitLinear()\n",
      "layers.18.self_attn.o_proj Ex4bitLinear()\n",
      "layers.18.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.18.mlp.gate_proj Ex4bitLinear()\n",
      "layers.18.mlp.up_proj Ex4bitLinear()\n",
      "layers.18.mlp.down_proj Ex4bitLinear()\n",
      "layers.18.mlp.act_fn SiLU()\n",
      "layers.18.input_layernorm ExLlamaRMSNorm()\n",
      "layers.18.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.19 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.19.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.19.self_attn.q_proj Ex4bitLinear()\n",
      "layers.19.self_attn.k_proj Ex4bitLinear()\n",
      "layers.19.self_attn.v_proj Ex4bitLinear()\n",
      "layers.19.self_attn.o_proj Ex4bitLinear()\n",
      "layers.19.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.19.mlp.gate_proj Ex4bitLinear()\n",
      "layers.19.mlp.up_proj Ex4bitLinear()\n",
      "layers.19.mlp.down_proj Ex4bitLinear()\n",
      "layers.19.mlp.act_fn SiLU()\n",
      "layers.19.input_layernorm ExLlamaRMSNorm()\n",
      "layers.19.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.20 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.20.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.20.self_attn.q_proj Ex4bitLinear()\n",
      "layers.20.self_attn.k_proj Ex4bitLinear()\n",
      "layers.20.self_attn.v_proj Ex4bitLinear()\n",
      "layers.20.self_attn.o_proj Ex4bitLinear()\n",
      "layers.20.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.20.mlp.gate_proj Ex4bitLinear()\n",
      "layers.20.mlp.up_proj Ex4bitLinear()\n",
      "layers.20.mlp.down_proj Ex4bitLinear()\n",
      "layers.20.mlp.act_fn SiLU()\n",
      "layers.20.input_layernorm ExLlamaRMSNorm()\n",
      "layers.20.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.21 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.21.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.21.self_attn.q_proj Ex4bitLinear()\n",
      "layers.21.self_attn.k_proj Ex4bitLinear()\n",
      "layers.21.self_attn.v_proj Ex4bitLinear()\n",
      "layers.21.self_attn.o_proj Ex4bitLinear()\n",
      "layers.21.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.21.mlp.gate_proj Ex4bitLinear()\n",
      "layers.21.mlp.up_proj Ex4bitLinear()\n",
      "layers.21.mlp.down_proj Ex4bitLinear()\n",
      "layers.21.mlp.act_fn SiLU()\n",
      "layers.21.input_layernorm ExLlamaRMSNorm()\n",
      "layers.21.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.22 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.22.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.22.self_attn.q_proj Ex4bitLinear()\n",
      "layers.22.self_attn.k_proj Ex4bitLinear()\n",
      "layers.22.self_attn.v_proj Ex4bitLinear()\n",
      "layers.22.self_attn.o_proj Ex4bitLinear()\n",
      "layers.22.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.22.mlp.gate_proj Ex4bitLinear()\n",
      "layers.22.mlp.up_proj Ex4bitLinear()\n",
      "layers.22.mlp.down_proj Ex4bitLinear()\n",
      "layers.22.mlp.act_fn SiLU()\n",
      "layers.22.input_layernorm ExLlamaRMSNorm()\n",
      "layers.22.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.23 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.23.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.23.self_attn.q_proj Ex4bitLinear()\n",
      "layers.23.self_attn.k_proj Ex4bitLinear()\n",
      "layers.23.self_attn.v_proj Ex4bitLinear()\n",
      "layers.23.self_attn.o_proj Ex4bitLinear()\n",
      "layers.23.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.23.mlp.gate_proj Ex4bitLinear()\n",
      "layers.23.mlp.up_proj Ex4bitLinear()\n",
      "layers.23.mlp.down_proj Ex4bitLinear()\n",
      "layers.23.mlp.act_fn SiLU()\n",
      "layers.23.input_layernorm ExLlamaRMSNorm()\n",
      "layers.23.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.24 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.24.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.24.self_attn.q_proj Ex4bitLinear()\n",
      "layers.24.self_attn.k_proj Ex4bitLinear()\n",
      "layers.24.self_attn.v_proj Ex4bitLinear()\n",
      "layers.24.self_attn.o_proj Ex4bitLinear()\n",
      "layers.24.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.24.mlp.gate_proj Ex4bitLinear()\n",
      "layers.24.mlp.up_proj Ex4bitLinear()\n",
      "layers.24.mlp.down_proj Ex4bitLinear()\n",
      "layers.24.mlp.act_fn SiLU()\n",
      "layers.24.input_layernorm ExLlamaRMSNorm()\n",
      "layers.24.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.25 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.25.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.25.self_attn.q_proj Ex4bitLinear()\n",
      "layers.25.self_attn.k_proj Ex4bitLinear()\n",
      "layers.25.self_attn.v_proj Ex4bitLinear()\n",
      "layers.25.self_attn.o_proj Ex4bitLinear()\n",
      "layers.25.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.25.mlp.gate_proj Ex4bitLinear()\n",
      "layers.25.mlp.up_proj Ex4bitLinear()\n",
      "layers.25.mlp.down_proj Ex4bitLinear()\n",
      "layers.25.mlp.act_fn SiLU()\n",
      "layers.25.input_layernorm ExLlamaRMSNorm()\n",
      "layers.25.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.26 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.26.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.26.self_attn.q_proj Ex4bitLinear()\n",
      "layers.26.self_attn.k_proj Ex4bitLinear()\n",
      "layers.26.self_attn.v_proj Ex4bitLinear()\n",
      "layers.26.self_attn.o_proj Ex4bitLinear()\n",
      "layers.26.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.26.mlp.gate_proj Ex4bitLinear()\n",
      "layers.26.mlp.up_proj Ex4bitLinear()\n",
      "layers.26.mlp.down_proj Ex4bitLinear()\n",
      "layers.26.mlp.act_fn SiLU()\n",
      "layers.26.input_layernorm ExLlamaRMSNorm()\n",
      "layers.26.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.27 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.27.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.27.self_attn.q_proj Ex4bitLinear()\n",
      "layers.27.self_attn.k_proj Ex4bitLinear()\n",
      "layers.27.self_attn.v_proj Ex4bitLinear()\n",
      "layers.27.self_attn.o_proj Ex4bitLinear()\n",
      "layers.27.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.27.mlp.gate_proj Ex4bitLinear()\n",
      "layers.27.mlp.up_proj Ex4bitLinear()\n",
      "layers.27.mlp.down_proj Ex4bitLinear()\n",
      "layers.27.mlp.act_fn SiLU()\n",
      "layers.27.input_layernorm ExLlamaRMSNorm()\n",
      "layers.27.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.28 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.28.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.28.self_attn.q_proj Ex4bitLinear()\n",
      "layers.28.self_attn.k_proj Ex4bitLinear()\n",
      "layers.28.self_attn.v_proj Ex4bitLinear()\n",
      "layers.28.self_attn.o_proj Ex4bitLinear()\n",
      "layers.28.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.28.mlp.gate_proj Ex4bitLinear()\n",
      "layers.28.mlp.up_proj Ex4bitLinear()\n",
      "layers.28.mlp.down_proj Ex4bitLinear()\n",
      "layers.28.mlp.act_fn SiLU()\n",
      "layers.28.input_layernorm ExLlamaRMSNorm()\n",
      "layers.28.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.29 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.29.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.29.self_attn.q_proj Ex4bitLinear()\n",
      "layers.29.self_attn.k_proj Ex4bitLinear()\n",
      "layers.29.self_attn.v_proj Ex4bitLinear()\n",
      "layers.29.self_attn.o_proj Ex4bitLinear()\n",
      "layers.29.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.29.mlp.gate_proj Ex4bitLinear()\n",
      "layers.29.mlp.up_proj Ex4bitLinear()\n",
      "layers.29.mlp.down_proj Ex4bitLinear()\n",
      "layers.29.mlp.act_fn SiLU()\n",
      "layers.29.input_layernorm ExLlamaRMSNorm()\n",
      "layers.29.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.30 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.30.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.30.self_attn.q_proj Ex4bitLinear()\n",
      "layers.30.self_attn.k_proj Ex4bitLinear()\n",
      "layers.30.self_attn.v_proj Ex4bitLinear()\n",
      "layers.30.self_attn.o_proj Ex4bitLinear()\n",
      "layers.30.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.30.mlp.gate_proj Ex4bitLinear()\n",
      "layers.30.mlp.up_proj Ex4bitLinear()\n",
      "layers.30.mlp.down_proj Ex4bitLinear()\n",
      "layers.30.mlp.act_fn SiLU()\n",
      "layers.30.input_layernorm ExLlamaRMSNorm()\n",
      "layers.30.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.31 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.31.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.31.self_attn.q_proj Ex4bitLinear()\n",
      "layers.31.self_attn.k_proj Ex4bitLinear()\n",
      "layers.31.self_attn.v_proj Ex4bitLinear()\n",
      "layers.31.self_attn.o_proj Ex4bitLinear()\n",
      "layers.31.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.31.mlp.gate_proj Ex4bitLinear()\n",
      "layers.31.mlp.up_proj Ex4bitLinear()\n",
      "layers.31.mlp.down_proj Ex4bitLinear()\n",
      "layers.31.mlp.act_fn SiLU()\n",
      "layers.31.input_layernorm ExLlamaRMSNorm()\n",
      "layers.31.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.32 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.32.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.32.self_attn.q_proj Ex4bitLinear()\n",
      "layers.32.self_attn.k_proj Ex4bitLinear()\n",
      "layers.32.self_attn.v_proj Ex4bitLinear()\n",
      "layers.32.self_attn.o_proj Ex4bitLinear()\n",
      "layers.32.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.32.mlp.gate_proj Ex4bitLinear()\n",
      "layers.32.mlp.up_proj Ex4bitLinear()\n",
      "layers.32.mlp.down_proj Ex4bitLinear()\n",
      "layers.32.mlp.act_fn SiLU()\n",
      "layers.32.input_layernorm ExLlamaRMSNorm()\n",
      "layers.32.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.33 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.33.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.33.self_attn.q_proj Ex4bitLinear()\n",
      "layers.33.self_attn.k_proj Ex4bitLinear()\n",
      "layers.33.self_attn.v_proj Ex4bitLinear()\n",
      "layers.33.self_attn.o_proj Ex4bitLinear()\n",
      "layers.33.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.33.mlp.gate_proj Ex4bitLinear()\n",
      "layers.33.mlp.up_proj Ex4bitLinear()\n",
      "layers.33.mlp.down_proj Ex4bitLinear()\n",
      "layers.33.mlp.act_fn SiLU()\n",
      "layers.33.input_layernorm ExLlamaRMSNorm()\n",
      "layers.33.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.34 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.34.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.34.self_attn.q_proj Ex4bitLinear()\n",
      "layers.34.self_attn.k_proj Ex4bitLinear()\n",
      "layers.34.self_attn.v_proj Ex4bitLinear()\n",
      "layers.34.self_attn.o_proj Ex4bitLinear()\n",
      "layers.34.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.34.mlp.gate_proj Ex4bitLinear()\n",
      "layers.34.mlp.up_proj Ex4bitLinear()\n",
      "layers.34.mlp.down_proj Ex4bitLinear()\n",
      "layers.34.mlp.act_fn SiLU()\n",
      "layers.34.input_layernorm ExLlamaRMSNorm()\n",
      "layers.34.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.35 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.35.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.35.self_attn.q_proj Ex4bitLinear()\n",
      "layers.35.self_attn.k_proj Ex4bitLinear()\n",
      "layers.35.self_attn.v_proj Ex4bitLinear()\n",
      "layers.35.self_attn.o_proj Ex4bitLinear()\n",
      "layers.35.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.35.mlp.gate_proj Ex4bitLinear()\n",
      "layers.35.mlp.up_proj Ex4bitLinear()\n",
      "layers.35.mlp.down_proj Ex4bitLinear()\n",
      "layers.35.mlp.act_fn SiLU()\n",
      "layers.35.input_layernorm ExLlamaRMSNorm()\n",
      "layers.35.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.36 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.36.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.36.self_attn.q_proj Ex4bitLinear()\n",
      "layers.36.self_attn.k_proj Ex4bitLinear()\n",
      "layers.36.self_attn.v_proj Ex4bitLinear()\n",
      "layers.36.self_attn.o_proj Ex4bitLinear()\n",
      "layers.36.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.36.mlp.gate_proj Ex4bitLinear()\n",
      "layers.36.mlp.up_proj Ex4bitLinear()\n",
      "layers.36.mlp.down_proj Ex4bitLinear()\n",
      "layers.36.mlp.act_fn SiLU()\n",
      "layers.36.input_layernorm ExLlamaRMSNorm()\n",
      "layers.36.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.37 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.37.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.37.self_attn.q_proj Ex4bitLinear()\n",
      "layers.37.self_attn.k_proj Ex4bitLinear()\n",
      "layers.37.self_attn.v_proj Ex4bitLinear()\n",
      "layers.37.self_attn.o_proj Ex4bitLinear()\n",
      "layers.37.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.37.mlp.gate_proj Ex4bitLinear()\n",
      "layers.37.mlp.up_proj Ex4bitLinear()\n",
      "layers.37.mlp.down_proj Ex4bitLinear()\n",
      "layers.37.mlp.act_fn SiLU()\n",
      "layers.37.input_layernorm ExLlamaRMSNorm()\n",
      "layers.37.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.38 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.38.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.38.self_attn.q_proj Ex4bitLinear()\n",
      "layers.38.self_attn.k_proj Ex4bitLinear()\n",
      "layers.38.self_attn.v_proj Ex4bitLinear()\n",
      "layers.38.self_attn.o_proj Ex4bitLinear()\n",
      "layers.38.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.38.mlp.gate_proj Ex4bitLinear()\n",
      "layers.38.mlp.up_proj Ex4bitLinear()\n",
      "layers.38.mlp.down_proj Ex4bitLinear()\n",
      "layers.38.mlp.act_fn SiLU()\n",
      "layers.38.input_layernorm ExLlamaRMSNorm()\n",
      "layers.38.post_attention_layernorm ExLlamaRMSNorm()\n",
      "layers.39 ExLlamaDecoderLayer(\n",
      "  (self_attn): ExLlamaAttention(\n",
      "    (q_proj): Ex4bitLinear()\n",
      "    (k_proj): Ex4bitLinear()\n",
      "    (v_proj): Ex4bitLinear()\n",
      "    (o_proj): Ex4bitLinear()\n",
      "  )\n",
      "  (mlp): ExLlamaMLP(\n",
      "    (gate_proj): Ex4bitLinear()\n",
      "    (up_proj): Ex4bitLinear()\n",
      "    (down_proj): Ex4bitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): ExLlamaRMSNorm()\n",
      "  (post_attention_layernorm): ExLlamaRMSNorm()\n",
      ")\n",
      "layers.39.self_attn ExLlamaAttention(\n",
      "  (q_proj): Ex4bitLinear()\n",
      "  (k_proj): Ex4bitLinear()\n",
      "  (v_proj): Ex4bitLinear()\n",
      "  (o_proj): Ex4bitLinear()\n",
      ")\n",
      "layers.39.self_attn.q_proj Ex4bitLinear()\n",
      "layers.39.self_attn.k_proj Ex4bitLinear()\n",
      "layers.39.self_attn.v_proj Ex4bitLinear()\n",
      "layers.39.self_attn.o_proj Ex4bitLinear()\n",
      "layers.39.mlp ExLlamaMLP(\n",
      "  (gate_proj): Ex4bitLinear()\n",
      "  (up_proj): Ex4bitLinear()\n",
      "  (down_proj): Ex4bitLinear()\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.39.mlp.gate_proj Ex4bitLinear()\n",
      "layers.39.mlp.up_proj Ex4bitLinear()\n",
      "layers.39.mlp.down_proj Ex4bitLinear()\n",
      "layers.39.mlp.act_fn SiLU()\n",
      "layers.39.input_layernorm ExLlamaRMSNorm()\n",
      "layers.39.post_attention_layernorm ExLlamaRMSNorm()\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the modules in the model\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
